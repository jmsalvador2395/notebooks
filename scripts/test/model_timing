#!/usr/bin/env python

#import torch
import time
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
import gc


if __name__ == '__main__':
    proj_dir = '/media/john/hdd01/projects/notebooks'
    cache_dir=f"{proj_dir}/cache/models"
    models = [
        #"meta-llama/Llama-3.2-1B-Instruct",
        #"meta-llama/Llama-3.2-3B-Instruct",
        "meta-llama/Llama-3.1-8B-Instruct",
        "meta-llama/Llama-3.1-70B-Instruct",
    ]

    messages = [
        {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
        {"role": "user", "content": "Who are you?"},
    ]

    # for model_name in tqdm(models, desc="Timing models"):
    for model_name in models:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True,
            device_map="cpu",
        )
        tok = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        pipeline = transformers.pipeline(
            "text-generation",
            model=model,
            tokenizer=tok,
        )
        
        start = time.time()
        outputs = pipeline(messages, max_new_tokens=256)
        end = time.time()
          
        print(f"Model: {model_name}, Time taken: {end - start} seconds")

        del pipeline
        del model
        del tok
        gc.collect()