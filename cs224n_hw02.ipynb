{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f9cbd1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f71c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import det\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3b6c5",
   "metadata": {},
   "source": [
    "# Gradient Comparisons\n",
    "\n",
    "# want gradient of  $-\\log{\\frac{\\exp{u_o^T v_c}}{\\sum\\limits_w \\exp{u_w^T v_c}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2cd7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw: tensor([[ 1.8423,  0.5189, -1.7119, -1.7014, -0.1297],\n",
      "        [-0.6018,  0.1450, -0.1498,  2.6146, -0.4340],\n",
      "        [ 0.3523, -0.0646,  1.4829,  0.4940,  0.2492],\n",
      "        [ 1.7470,  0.7448,  0.0317, -1.1724, -1.5069]], requires_grad=True)\n",
      "uo: tensor([-0.6018,  0.1450, -0.1498,  2.6146, -0.4340],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "vc: tensor([ 0.0571, -1.1894, -0.5659, -0.8327,  0.9014], requires_grad=True)\n",
      "\n",
      "torch gradient: tensor([ 2.3247,  0.3454, -1.2724, -4.1145,  0.2599])\n",
      "numpy gradient 1: [ 2.3246808   0.34541243 -1.2724466  -4.1145277   0.25991312]\n",
      "numpy gradient 2: [ 2.3246808   0.34541243 -1.2724466  -4.1145277   0.25991312]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "uw=torch.randn(4,5, requires_grad=True)\n",
    "uo=uw[1]\n",
    "vc=torch.randn(5, requires_grad=True)\n",
    "\n",
    "print(f'uw: {uw}')\n",
    "print(f'uo: {uo}')\n",
    "print(f'vc: {vc}')\n",
    "\n",
    "loss=-torch.log(torch.exp(uo@vc)/torch.sum(torch.exp(uw@vc)))\n",
    "loss.backward()\n",
    "print()\n",
    "print(f'torch gradient: {vc.grad}')\n",
    "\n",
    "uw=uw.detach().numpy()\n",
    "uo=uo.detach().numpy()\n",
    "vc=vc.detach().numpy()\n",
    "\n",
    "e_wv=np.exp(uw@vc)\n",
    "e_ov=np.exp(vc@uo)\n",
    "\n",
    "left=uo*np.sum(e_wv)\n",
    "right=np.sum(uw*e_wv[:, None] ,axis=0)\n",
    "bottom=np.sum(e_wv)\n",
    "\n",
    "#grad1=-(np.sum(e_wv)/e_ov)*((left-right)/bottom)\n",
    "grad1=-(left-right)/bottom\n",
    "grad2=-((uo*np.sum(np.exp(vc@uw.T))-np.sum(uw*np.exp(vc@uw.T)[:, None] ,axis=0))/np.sum(np.exp(vc@uw.T)))\n",
    "\n",
    "print(f'numpy gradient 1: {grad1}')\n",
    "print(f'numpy gradient 2: {grad2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1c256",
   "metadata": {},
   "source": [
    "# Checking Basic Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa805fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res:  tensor(4.5714, grad_fn=<NegBackward0>)\n",
      "grad:  tensor([ 2.3247,  0.3454, -1.2724, -4.1145,  0.2599])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "uo_idx=1\n",
    "uw=torch.randn(4,5, requires_grad=True)\n",
    "uo=uw[uo_idx]\n",
    "vc=torch.randn(5, requires_grad=True)\n",
    "\n",
    "def softmax_np(w, v):\n",
    "    return np.exp(w@v)/np.sum(np.exp(w@v))\n",
    "\n",
    "def softmax_torch(w, v):\n",
    "    return torch.exp(w@v)/torch.sum(torch.exp(w@v))\n",
    "\n",
    "g=-torch.log(softmax_torch(uw, vc)[uo_idx])\n",
    "print('res: ', g)\n",
    "g.backward()\n",
    "print('grad: ', vc.grad)\n",
    "\n",
    "vc=vc.detach().numpy()\n",
    "uw=uw.detach().numpy()\n",
    "\n",
    "#print(f'guess:, {guess}')\n",
    "\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8898f7",
   "metadata": {},
   "source": [
    "# Naive Softmax Loss\n",
    "$P(O=o|C=c)=\\frac{exp(u_o^T v_c)}{\\sum\\limits_{w \\in Vocab}\\exp(u_w^T vc)}$  \n",
    "$J_{naive-softmax}(v_c, o, U)=-\\log P(O=o|C=c)$\n",
    "\n",
    "# Cross-Enropy\n",
    "$-\\sum\\limits_{w \\in Vocab}y_w\\log(\\hat{y}_w)=-\\log(\\hat{y}_o)$\n",
    "\n",
    "# Prove that the naive-softmax loss is the same as cross entropy loss between **$y$** and **$\\hat{y}$** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63859146",
   "metadata": {},
   "source": [
    "We already have the center word so the probability $y_{w}$ is 1 so the output of the whole lefthand side is the $i$th value which corresponds to $\\hat{y}_o$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0df50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 3, 2, 5]\n",
      "tensor([-3.5009,  0.3441, -1.2571], dtype=torch.float64)\n",
      "[-3.50085339  0.34406705 -1.25712608]\n"
     ]
    }
   ],
   "source": [
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "vc=np.random.randn(3)\n",
    "uw=np.random.randn(7,3)\n",
    "sample_idx=[0, 0, 3, 2, 5]\n",
    "neg_samples=uw[sample_idx]\n",
    "print(sample_idx)\n",
    "idx=1\n",
    "\n",
    "t_vc=torch.tensor(vc, requires_grad=True)\n",
    "t_uw=torch.tensor(uw, requires_grad=True)\n",
    "t_neg_samples=torch.tensor(neg_samples, requires_grad=True)\n",
    "\n",
    "\n",
    "t_ans=-torch.log(torch.sigmoid(t_uw[idx]@t_vc))-torch.sum(torch.log(torch.sigmoid(-t_neg_samples@t_vc)))\n",
    "t_ans.backward()\n",
    "print(t_vc.grad)\n",
    "\n",
    "guess=(\n",
    "    (sigmoid(uw[idx]@vc)-1)*uw[idx]+\n",
    "    np.sum(-neg_samples*(sigmoid(-neg_samples@vc)-1)[:, None], axis=0)\n",
    ")\n",
    "print(guess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
