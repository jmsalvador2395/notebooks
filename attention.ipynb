{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151f5b63-b900-4dc6-ba44-1ee211e86fbf",
   "metadata": {},
   "source": [
    "# Call Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b1ae58-effa-4fa5-9036-f5423d6eafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09124bfa-6599-4984-af87-ec1bd63cb113",
   "metadata": {},
   "source": [
    "# Make Random Q, K, and V tensors\n",
    "\n",
    "`T` for timestep  \n",
    "`d` for dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9324a94d-484c-4384-8f5d-f1ef2e94a212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape: torch.Size([3, 4])\n",
      "k shape: torch.Size([3, 4])\n",
      "v shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "T, d = 3, 4\n",
    "q, k, v = torch.rand((T, d*3)).chunk(3, dim=-1)\n",
    "\n",
    "print(f'q shape: {q.shape}')\n",
    "print(f'k shape: {k.shape}')\n",
    "print(f'v shape: {v.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6614a1-4ad4-42ba-b3c2-adb0653c1ea1",
   "metadata": {},
   "source": [
    "# Compute Attention (With Causal Mask)\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d}}\\right)V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53036284-3dfc-4420-a307-5614d064aa6c",
   "metadata": {},
   "source": [
    "# First Compute $\\frac{QK^{\\top}}{\\sqrt{d}}$\n",
    "\n",
    "# Outputs a (T, T) matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4548c258-90a8-4e24-8ad0-8fe61ab07d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0366, 0.4480, 0.8684],\n",
      "        [0.6112, 0.2692, 0.4616],\n",
      "        [0.3284, 0.1526, 0.4017]])\n"
     ]
    }
   ],
   "source": [
    "qk = (q@k.T)/np.sqrt(d)\n",
    "\n",
    "print(qk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecbe83e-216e-4e72-a920-28f9de53826b",
   "metadata": {},
   "source": [
    "# Masking is applied by setting masked values to $-\\infty$ before softmax\n",
    "$\\oslash$ means element-wise division\n",
    "$$\n",
    "\\text{softmax}(x) = \\exp(x) \\oslash \\sum\\limits_{i}\\exp(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e7a375-3101-4314-bd93-1cf85243545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: tensor([0.1602, 0.6989, 0.0781, 0.5008, 0.2713])\n",
      "\n",
      "v with masked end value: tensor([0.1602, 0.6989, 0.0781, 0.5008,   -inf])\n",
      "\n",
      "apply softmax: tensor([0.1984, 0.3400, 0.1827, 0.2789, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# random example vector\n",
    "example = torch.rand(5)\n",
    "print(f'v: {example}')\n",
    "\n",
    "# Set last value to -inf\n",
    "example[-1] = float('-inf')\n",
    "print(f'\\nv with masked end value: {example}')\n",
    "\n",
    "# apply softmax after setting last value to -inf\n",
    "s_example = torch.softmax(example, dim=0)\n",
    "print(f'\\napply softmax: {s_example}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fae445-a170-4c0e-ab6d-20368736ce2b",
   "metadata": {},
   "source": [
    "# Now Apply Causal Attention Mask and Compute Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d996e8-4ac4-49cd-b12b-1344f1143fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask (True values are the ones that get masked out):\n",
      "tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "\n",
      "QK^T/(sqrt(d)) after applying softmax:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5847, 0.4153, 0.0000],\n",
      "        [0.3431, 0.2878, 0.3692]])\n"
     ]
    }
   ],
   "source": [
    "# Create attention mask\n",
    "attn_mask = torch.triu(\n",
    "    torch.ones((T, T), dtype=torch.bool),\n",
    "    diagonal=1\n",
    ")\n",
    "print('Attention mask (True values are the ones that get masked out):')\n",
    "print(attn_mask)\n",
    "\n",
    "# Apply attention mask\n",
    "sqk = qk.clone()\n",
    "sqk[attn_mask] = float('-inf')\n",
    "sqk = torch.softmax(sqk, dim=-1)\n",
    "print('\\nQK^T/(sqrt(d)) after applying softmax:')\n",
    "print(sqk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c6f6e-4674-430b-acbe-d67bc71cb672",
   "metadata": {},
   "source": [
    "# Now Matrix Multiply with V and you have your attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34380181-94f5-49a7-93a1-e96d266ecd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "tensor([[0.2759, 0.8454, 0.4397, 0.1585],\n",
      "        [0.4682, 0.7469, 0.2908, 0.4935],\n",
      "        [0.6005, 0.5586, 0.2971, 0.6820]])\n"
     ]
    }
   ],
   "source": [
    "attn_out = sqk@v\n",
    "\n",
    "print('output:')\n",
    "print(attn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27304eb7-df9a-43d4-a9ed-6ad90cfd2240",
   "metadata": {},
   "source": [
    "# Compare with Original V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74209f38-4df9-4f2a-a6b1-575a02633256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V:\n",
      "tensor([[0.2759, 0.8454, 0.4397, 0.1585],\n",
      "        [0.7389, 0.6082, 0.0813, 0.9651],\n",
      "        [0.7942, 0.2534, 0.3329, 0.9478]])\n",
      "\n",
      "QK^T/(sqrt(d)) after applying softmax:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5847, 0.4153, 0.0000],\n",
      "        [0.3431, 0.2878, 0.3692]])\n",
      "\n",
      "Attention Output:\n",
      "tensor([[0.2759, 0.8454, 0.4397, 0.1585],\n",
      "        [0.4682, 0.7469, 0.2908, 0.4935],\n",
      "        [0.6005, 0.5586, 0.2971, 0.6820]])\n"
     ]
    }
   ],
   "source": [
    "print('V:')\n",
    "print(v)\n",
    "\n",
    "print('\\nQK^T/(sqrt(d)) after applying softmax:')\n",
    "print(sqk)\n",
    "\n",
    "print('\\nAttention Output:')\n",
    "print(attn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35658a-2c4b-4065-9a78-afea895ef745",
   "metadata": {},
   "source": [
    "# Notice that row 0 of attention output is just the first value vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdd5398f-fe2a-42c5-b610-fe977182d565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_out[0] == v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c57c8c-5c12-42c7-b99d-5424cc10d3c2",
   "metadata": {},
   "source": [
    "# Now see that row 1 of attention output is just a linear combination of the first 2 rows of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399d39bc-2ec6-4210-a074-c3dee60ab51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare extracted output with manual computation\n",
    "(sqk@v)[1] == sqk[1, 0]*v[0] + sqk[1, 1]*v[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2f886-ed8d-4012-aafb-89bb7ebf0966",
   "metadata": {},
   "source": [
    "# Now see that row 2 of attention output is just a linear combination of all 3 rows of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15e9267-ee6b-4df4-8961-b773f7a886be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sqk@v)[2] == sqk[2, 0]*v[0] + sqk[2, 1]*v[1] + sqk[2, 2]*v[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
