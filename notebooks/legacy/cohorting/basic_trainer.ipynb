{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d6ae67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import transformers\n",
    "\n",
    "#from nbtools.utils import files\n",
    "#proj_dir = files.project_root()\n",
    "proj_dir = '/media/john/hdd01/projects/notebooks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c327a68",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cfbd99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of cohorts that map to census: 0\n",
      "# of cohorts that map to diagnoses: 23\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "data_pth = f'{proj_dir}/data/census_data.xlsx'\n",
    "census = pd.read_excel(data_pth, sheet_name='Census (Stays)')\n",
    "diagnoses = pd.read_excel(data_pth, sheet_name='DXes')\n",
    "cohorts = pd.read_excel(data_pth, sheet_name='Cohorts')\n",
    "\n",
    "# perform inner join to see how many records match\n",
    "coh_cen = pd.merge(\n",
    "    cohorts, census, how='inner', on='RESIDENT ID'\n",
    ")\n",
    "coh_diag = pd.merge(\n",
    "    cohorts, diagnoses, how='inner', on='RESIDENT ID'\n",
    ")\n",
    "\n",
    "print(f'# of cohorts that map to census: {len(coh_cen)}')\n",
    "print(f'# of cohorts that map to diagnoses: {len(coh_diag)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae826184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ADMISS. ID, FACILITY ID_x, RESIDENT ID_x, START DATE_x, END DATE_x, START REASON, SOURCE, SOURCE TYPE, ADMISSION ACTION DESC, ADMISSION ACTION CODE, END REASON, END REASON CODE, DESTINATION, DESTINATION TYPE, HOSPITALIZATION REASON_x, FACILITY ID_y, RESIDENT ID_y, START DATE_y, END DATE_y, HOSPITALIZATION REASON_y, PMTS COHORT]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "census_cohort = pd.merge(\n",
    "    census, cohorts, how='inner', on='ADMISS. ID'\n",
    ")\n",
    "\n",
    "print(census_cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "603bcf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "censet = set(census['ADMISS. ID'])\n",
    "cohset = set(cohorts['ADMISS. ID'])\n",
    "print(censet.intersection(cohset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0d98d",
   "metadata": {},
   "source": [
    "# Define a new model using BERT to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92007b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(SentimentAnalyzer, self).__init__()\n",
    "\n",
    "        # pretrained BERT model\n",
    "        self.bert = transformers.BertModel.from_pretrained(\n",
    "            'bert-base-uncased'\n",
    "        )\n",
    "\n",
    "        # classification head\n",
    "        self.classifier = nn.Linear(768, n_class)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        out1 = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        return self.classifier(out1.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03556602",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = transformers.BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased'\n",
    ")\n",
    "model = SentimentAnalyzer(n_class=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e06d405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2023, 2003, 1037, 7099, 6251, 2005, 5604, 1012,  102]])\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is a sample sentence for testing.\"\n",
    "inputs = tok(\n",
    "    sample_text,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d8d1cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2120,  0.3233]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": 25.817,\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": 69.560,\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\": 120.867,\n",
    "    \"meta-llama/Llama-3.1-70B-Instruct\": 0,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
