{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandas\n",
    "from datasets import (\n",
    "    load_dataset, Dataset, DatasetDict, concatenate_datasets,\n",
    ")\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102fd92bccb04ec7bf0b2fda985222ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/78528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6001f2aecd8a4013a25da7569b195e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c2d3b743ff4ee38e1e748cc07bea99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 78528\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 9816\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 9817\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def transform(sample):\n",
    "    sents = [sample[f'sentence{i}'] for i in range(1, 6)]\n",
    "    return {'sentences': sents}\n",
    "base_path = '/data/john/datasets/ROCStories/'\n",
    "ds = load_dataset(base_path)\n",
    "\n",
    "train_test = ds['train'].train_test_split(test_size=0.2)\n",
    "test_val = train_test['test'].train_test_split(test_size=0.5)\n",
    "roc = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': test_val['train'],\n",
    "    'test': test_val['test'],\n",
    "})\n",
    "#roc = roc.with_transform(transform)\n",
    "roc = roc.map(transform).select_columns('sentences')\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4856c15d1610406d97beeede4e8a06bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ad9669d57b4f81a3c51c9ec39b81dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212c304114374eb885f2853b6f5bc754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 40155\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 4990\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 5055\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "sind_base = f'/data/john/datasets/sind'\n",
    "sind_files = {\n",
    "    'train': f'{sind_base}/train.tsv',\n",
    "    'validation': f'{sind_base}/dev.tsv',\n",
    "    'test': f'{sind_base}/test.tsv',\n",
    "}\n",
    "sind = load_dataset(\n",
    "    'text', data_files=sind_files,\n",
    ")\n",
    "def sind_xform(sample):\n",
    "    sents = sample['text'].split('<eos>')\n",
    "    sents = [sent.strip() for sent in sents]\n",
    "    return {'sentences': sents}\n",
    "#sind = sind.with_transform(sind_xform)\n",
    "sind = sind.map(sind_xform).select_columns('sentences')\n",
    "print(sind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset at position 0 has at least one split: ['train', 'validation', 'test']\nPlease pick one to interleave with the other datasets, for example: dataset['train']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mroc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/john/projects/nb/env/lib/python3.11/site-packages/datasets/combine.py:197\u001b[0m, in \u001b[0;36mconcatenate_datasets\u001b[0;34m(dsets, info, split, axis)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset:\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a list of Dataset objects or a list of IterableDataset objects, but element at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis an empty dataset dictionary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has at least one split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pick one to interleave with the other datasets, for example: dataset[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a list of Dataset objects or a list of IterableDataset objects, but element at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset at position 0 has at least one split: ['train', 'validation', 'test']\nPlease pick one to interleave with the other datasets, for example: dataset['train']"
     ]
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    split: concatenate_datasets([roc[split], sind[split]])\n",
    "    for split in ['train', 'test', 'validation']\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
